{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-28 17:26:03,326 loading file C:\\Users\\97254\\.flair\\models\\sentiment-en-mix-distillbert_3.1.pt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "from newspaper import Config\n",
    "import logging\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from timeit import default_timer as timer\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "import neuralcoref\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from collections import defaultdict\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "from segtok.segmenter import split_single\n",
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_lg\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "nlp = en_core_web_lg.load()\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "classifier = TextClassifier.load('en-sentiment')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configurations\n",
    "headers = {\n",
    "    'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.183 Safari/537.36\"}\n",
    "config = Config()\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.183 Safari/537.36\"\n",
    "config.browser_user_agent = user_agent\n",
    "logging.basicConfig(filename='Finviz_log.txt',\n",
    "                   filemode='a',\n",
    "                   format='%(asctime)s,%(msecs)d %(levelname)s %(message)s',\n",
    "                   datefmt='%H:%M:%S',\n",
    "                   level=logging.INFO)\n",
    "base_path= r'C:\\Users\\97254\\anaconda3\\finbert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Articles Functions\"\"\"\n",
    "def get_article(ticker):\n",
    "    baseURL=f\"https://finviz.com/quote.ashx?t={ticker}\"\n",
    "    response = requests.get(baseURL,headers=headers)\n",
    "    article_elements = BeautifulSoup(response.content.decode(),features='html.parser').find_all('a',{\"class\":\"tab-link-news\"},href=True)\n",
    "    if article_elements:\n",
    "        return [element['href'] for element in article_elements]\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "def get_article_headline(soup,articleURL):\n",
    "    try:\n",
    "        text = soup.find('h1').get_text(separator=\" \")\n",
    "        headln = re.sub('[ג€™]|[ג]|[\\x9c]|[\\xa0]|[\\n]', '', text)\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Failed: {e}-- Could not find Headline |{articleURL}\")\n",
    "        headln = ''\n",
    "    return headln\n",
    "\n",
    "def get_article_date(soup,articleURL):\n",
    "    for i in soup.findAll('time'):\n",
    "        if i.has_attr('datetime'):\n",
    "            return i['datetime'][:10]\n",
    "    return 'None'\n",
    "\n",
    "def get_article_record_for_URL(articleURL,ticker):\n",
    "    response = requests.get(articleURL,headers=headers)\n",
    "    soup = BeautifulSoup(response.content.decode(),features='html.parser')\n",
    "    headln = get_article_headline(soup,articleURL)\n",
    "    article = Article(articleURL, config=config)\n",
    "    try:\n",
    "        article.download()\n",
    "        article.parse()\n",
    "    except Exception:\n",
    "        return {}\n",
    "    try:\n",
    "        pubdate = str(article.publish_date.date())\n",
    "    except Exception:\n",
    "        pubdate = get_article_date(soup,articleURL)\n",
    "    if pubdate == None or pubdate =='':\n",
    "        pubdate = get_article_date(soup,articleURL)\n",
    "    txt = article.text\n",
    "    if not txt:\n",
    "        logging.info(f\"Failed: Could not scrape article | {articleURL}\")\n",
    "        return {}\n",
    "    return {\"Headline\":headln,'Date':str(pubdate),'Text':txt}\n",
    "\n",
    "\"\"\"Misc Functions\"\"\"\n",
    "def get_symbol(ticker):\n",
    "    url = \"http://d.yimg.com/autoc.finance.yahoo.com/autoc?query={}&region=1&lang=en.json\".format(ticker)\n",
    "    result = requests.get(url).json()\n",
    "    for x in result['ResultSet']['Result']:\n",
    "        if x['symbol'] == ticker:\n",
    "            partitioned_string = x['name'].partition(' ')\n",
    "            company = partitioned_string[0]\n",
    "            return company\n",
    "        \n",
    "def chunk_list(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "        \n",
    "def get_weight(sentiment,lengths):\n",
    "    output=[]\n",
    "    weights=[]\n",
    "    for length in lengths:\n",
    "        weights.append(length/max(lengths))\n",
    "    output=[x*z if x!=np.nan else 0 for x,z in zip(sentiment,weights)]\n",
    "    return np.nanmean(output) \n",
    "\n",
    "def load_transformer_model(basepath):\n",
    "    torch.set_grad_enabled(False)\n",
    "    model_dir = basepath\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(model_dir)\n",
    "    return model, tokenizer\n",
    "\n",
    "\"\"\"Sentiment Functions\"\"\"\n",
    "def vaderPr(sentence):\n",
    "    if len(sentence)<4:\n",
    "        return 0\n",
    "    return analyzer.polarity_scores(sentence)['compound']    \n",
    "\n",
    "def flairPr(sentence):\n",
    "    if len(sentence)<4:\n",
    "        return 0\n",
    "    text = Sentence(sentence)\n",
    "    classifier.predict(text)\n",
    "    value=text.labels[0].to_dict()['value']\n",
    "    if value == 'POSITIVE':\n",
    "        result = text.to_dict()['labels'][0]['confidence']\n",
    "    else:\n",
    "        result = -(text.to_dict()['labels'][0]['confidence'])\n",
    "    return round(result,3)\n",
    "        \n",
    "def get_finbert_esg_sentiments_batch(sents, model, tokenizer,  batch_size=50):\n",
    "    labels = {0: 'neutral', 1: 'positive', 2: 'negative'}\n",
    "    scores = {'neutral': 0, 'positive': 1 , 'negative':-1}\n",
    "    pbar = tqdm(total=(len(sents) // batch_size), position=0, leave=True)\n",
    "    preds = []\n",
    "    confidence = []\n",
    "    sentiment=[]\n",
    "    for batch_of_sents in chunk_list(sents, batch_size):\n",
    "        inputs = tokenizer.batch_encode_plus(batch_of_sents, return_tensors='pt', padding=True, truncation=True)\n",
    "        preds_probs = model(inputs['input_ids'].to(device), token_type_ids=None, attention_mask=inputs['attention_mask'].to(device))[0]\n",
    "        torch.cuda.empty_cache()\n",
    "        for pp in preds_probs:\n",
    "            pp = softmax(pp, dim=0)\n",
    "            top = pp.topk(1)\n",
    "            idxs = top.indices.tolist()\n",
    "            wts = top.values.tolist()\n",
    "            preds.append(labels[idxs[0]])\n",
    "            confidence.append(wts[0])\n",
    "            sentiment.append(scores.get(labels[idxs[0]]))\n",
    "        pbar.update()\n",
    "    return sentiment, confidence\n",
    "\n",
    "\"\"\"Scraping and Analysis Functions\"\"\"\n",
    "def scrape_ticker (ticker):\n",
    "    s_time= timer()\n",
    "    logging.info(f\"Running Finviz Scraper on {ticker}\")\n",
    "    articles = get_article(ticker)\n",
    "    if os.path.isfile(f'./{ticker}.json'):\n",
    "        print('Historical Data Found')\n",
    "        with open(f'./{ticker}.json',encoding='utf-8') as f:\n",
    "            finviz=json.loads(f.read())\n",
    "        for article in tqdm(articles, position=0, leave=True):\n",
    "            article_dict = get_article_record_for_URL(article,ticker)\n",
    "            if not article_dict:\n",
    "                continue\n",
    "            else:\n",
    "                if article_dict['Date']=='None':\n",
    "                    continue\n",
    "                if str(article_dict['Date']) in list(finviz.keys()):\n",
    "                    if article in list(finviz[str(article_dict['Date'])].keys()):\n",
    "                        continue\n",
    "                    else:\n",
    "                        finviz[str(article_dict['Date'])].update({article:article_dict})\n",
    "                else:\n",
    "                    finviz[str(article_dict['Date'])] = defaultdict()\n",
    "                    finviz[str(article_dict['Date'])].update({article:article_dict})\n",
    "    else:\n",
    "        print('No Historical Data Found')\n",
    "        finviz = defaultdict()\n",
    "        for article in tqdm(articles, position=0, leave=True):\n",
    "            article_dict = get_article_record_for_URL(article,ticker)\n",
    "            if not article_dict:\n",
    "                continue\n",
    "            if article_dict['Date']=='None':\n",
    "                continue\n",
    "            else:\n",
    "                if str(article_dict['Date']) in list(finviz.keys()):\n",
    "                    finviz[str(article_dict['Date'])].update({article:article_dict}) \n",
    "                else:\n",
    "                    finviz[str(article_dict['Date'])] = defaultdict()\n",
    "                    finviz[str(article_dict['Date'])].update({article:article_dict})\n",
    "    with open(f'./{ticker}.json','w',encoding='utf-8') as f:\n",
    "        json.dump(finviz,f,ensure_ascii=False,indent=4,sort_keys=True)\n",
    "    logging.info(f\"Finished scraping {ticker} news from Finviz\")\n",
    "    print(f'Scraping done after {round((timer()-s_time)/60,2)} minutes')    \n",
    "    return finviz\n",
    "\n",
    "def analyze (ticker,data,classifier):\n",
    "    logging.info(f\"Running Finviz Scraper on {ticker} by {classifier}\")\n",
    "    classifiers = ['vader','flair','finbert']\n",
    "    if classifier not in classifiers:\n",
    "        logging.info(f\"Program stopped due to an invalid classifier\")\n",
    "        return print(f'Please select a valid classifier out of the following: {classifiers}')\n",
    "    s_time= timer()\n",
    "    finviz = data\n",
    "    print('Starting sentiment analysis process')\n",
    "    if classifier == 'finbert':\n",
    "        model,tokenizer = load_transformer_model(base_path)\n",
    "    if os.path.isfile(f'./{ticker}-{classifier}.json'):\n",
    "        print('Historical Analysis Found')\n",
    "        with open(f'./{ticker}-{classifier}.json',encoding='utf-8') as f:\n",
    "            final=json.loads(f.read())\n",
    "    else:\n",
    "        final = defaultdict()            \n",
    "    company = get_symbol(ticker)\n",
    "    queries = [ticker,company]\n",
    "    final[ticker] = defaultdict()\n",
    "    logging.info(f\"Starting analyzing {ticker} news by {classifier}\")\n",
    "    for date in tqdm(list(finviz.keys()), position=0, leave=True):\n",
    "        if date in list(final[ticker].keys()):\n",
    "            weights = []\n",
    "            mean_sent_list = []\n",
    "            for article in list(final[date].keys()):\n",
    "                weights.append(len([ticker][date][artDict][artDict][article]['Relevant']))\n",
    "                mean_sent_list.append([ticker][date][artDict][artDict][article]['Sentiment Score'])\n",
    "            for article in list(finviz[date].keys()):\n",
    "                if article not in list(final[date].keys()):\n",
    "                    sentiment_scores=[]\n",
    "                    content = article['Text']\n",
    "                    relevant=[]\n",
    "                    try:\n",
    "                        clean = re.sub('[ג€™]|[ג]|[\\x9c]|[\\xa0]|[\\n]', '', content)\n",
    "                        doc = nlp(clean)\n",
    "                    except Exception:\n",
    "                        doc = nlp(str(content))\n",
    "                    resolved_text = doc._.coref_resolved\n",
    "                    sentences = [sent.string.strip() for sent in nlp(resolved_text).sents]\n",
    "                    for query in queries:\n",
    "                        output = [sent for sent in sentences if str.lower(query) in \n",
    "                              (' '.join([token.lemma_.lower() for token in nlp(sent)]))]\n",
    "                        if len(output)==0:\n",
    "                            continue\n",
    "                        else:\n",
    "                            relevant.extend(output)\n",
    "                    if len(relevant)==0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        for sentence in relevant:\n",
    "                            if classifier == 'flair':\n",
    "                                sentiment_scores.append(flairPr(sentence))\n",
    "                            elif classifier == 'vader':\n",
    "                                sentiment_scores.append(vaderPr(sentence))\n",
    "                    if classifier == 'finbert':\n",
    "                        sentiment,confidence = get_finbert_esg_sentiments_batch(\n",
    "                            relevant, model, tokenizer,10)\n",
    "                        scores = [x*y for x,y in zip(confidence,sentiment)]\n",
    "                        sentiment_scores = round(np.mean(scores),3)   \n",
    "                    weights.append(len(relevant))\n",
    "                    mean_sent_list.append(np.nanmean(sentiment_scores))\n",
    "                    artDict[article]={\n",
    "                        'Article Title':article['Headline'],\n",
    "                        'Relevant Text':relevant,\n",
    "                        'Sentiment Score':np.nanmean(sentiment_scores)}\n",
    "        else:\n",
    "            weights = []\n",
    "            mean_sent_list = []\n",
    "            artDict= {}\n",
    "            for article in list(finviz[date].keys()):\n",
    "                sentiment_scores=[]\n",
    "                content = finviz[date][article]['Text']\n",
    "                relevant=[]\n",
    "                try:\n",
    "                    clean = re.sub('[ג€™]|[ג]|[\\x9c]|[\\xa0]|[\\n]', '', content)\n",
    "                    doc = nlp(clean)\n",
    "                except Exception:\n",
    "                    doc = nlp(str(content))\n",
    "                resolved_text = doc._.coref_resolved\n",
    "                sentences = [sent.string.strip() for sent in nlp(resolved_text).sents]\n",
    "                for query in queries:\n",
    "                    output = [sent for sent in sentences if str.lower(query) in \n",
    "                          (' '.join([token.lemma_.lower() for token in nlp(sent)]))]\n",
    "                    if len(output)==0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        for sentence in output:\n",
    "                            if sentence not in relevant:\n",
    "                                relevant.append(sentence)\n",
    "                if len(relevant)==0:\n",
    "                    continue\n",
    "                else:\n",
    "                    for sentence in relevant:\n",
    "                        if classifier == 'flair':\n",
    "                            sentiment_scores.append(flairPr(sentence))\n",
    "                        elif classifier == 'vader':\n",
    "                            sentiment_scores.append(vaderPr(sentence))\n",
    "                    if classifier == 'finbert':\n",
    "                        sentiment,confidence = get_finbert_esg_sentiments_batch(\n",
    "                            relevant, model, tokenizer,10)\n",
    "                        scores = [x*y for x,y in zip(confidence,sentiment)]\n",
    "                        sentiment_scores = round(np.mean(scores),3)   \n",
    "                    weights.append(len(relevant))\n",
    "                mean_sent_list.append(np.nanmean(sentiment_scores))\n",
    "                artDict[article]={\n",
    "                    'Article Title':finviz[date][article]['Headline'],\n",
    "                    'Relevant Text':relevant,\n",
    "                    'Sentiment Score':round(np.nanmean(sentiment_scores),3)}\n",
    "        if len(weights)==0:\n",
    "            continue\n",
    "        final[ticker][str(date)] ={\n",
    "            'Weighted Score':round(get_weight(mean_sent_list,weights),3),\n",
    "            'Articles':artDict}\n",
    "        logging.info(f\"Program finished analyzing\")\n",
    "    print(f'Program done after {round((timer()-s_time)/60,2)} minutes')\n",
    "    with open(f'./{ticker}-{classifier}.json','w',encoding='utf-8') as f:\n",
    "        json.dump(final,f,ensure_ascii=False,indent=4,sort_keys=True)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historical Data Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [03:34<00:00,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping done after 3.59 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "finviz = scrape_ticker ('BABA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sentiment analysis process\n",
      "Historical Analysis Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 42/42 [18:51<00:00, 26.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program done after 18.86 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "analyze('BABA',finviz,'flair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sentiment analysis process\n",
      "Historical Analysis Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 42/42 [17:49<00:00, 25.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program done after 17.84 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "analyze('BABA',finviz,'vader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sentiment analysis process\n",
      "Historical Analysis Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.88it/s]                                                                  | 2/42 [00:38<14:53, 22.34s/it]\n",
      "1it [00:00,  3.10it/s]\n",
      "1it [00:00,  1.34it/s]\n",
      "1it [00:00,  2.07it/s]\n",
      "1it [00:00,  4.91it/s]                                                                  | 7/42 [02:34<10:30, 18.01s/it]\n",
      "1it [00:00,  4.00it/s]\n",
      "1it [00:00,  3.27it/s]\n",
      "2it [00:00,  3.48it/s]                                                                                                 \n",
      "1it [00:00,  5.53it/s]\n",
      "1it [00:00,  2.84it/s]\n",
      "1it [00:00,  1.84it/s]\n",
      "1it [00:00,  3.36it/s]\n",
      "2it [00:03,  1.58s/it]                                                                                                 \n",
      "3it [00:03,  1.11s/it]                                                                                                 \n",
      "1it [00:00,  4.89it/s]██▌                                                              | 10/42 [05:55<25:34, 47.95s/it]\n",
      "2it [00:01,  1.67it/s]                                                                                                 \n",
      "1it [00:00,  2.75it/s]\n",
      "1it [00:00,  1.95it/s]████████▍                                                        | 13/42 [07:11<16:56, 35.04s/it]\n",
      "2it [00:01,  1.55it/s]                                                                                                 \n",
      "2it [00:00,  2.26it/s]                                                                                                 \n",
      "1it [00:00,  1.42it/s]██████████████████▏                                              | 18/42 [08:54<07:19, 18.32s/it]\n",
      "2it [00:01,  1.79it/s]                                                                                                 \n",
      "4it [00:05,  1.33s/it]                                                                                                 \n",
      "2it [00:00,  3.24it/s]                                                                                                 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.54it/s]\n",
      "1it [00:00, 12.51it/s]\n",
      "2it [00:01,  1.80it/s]                                                                                                 \n",
      "2it [00:01,  1.35it/s]                                                                                                 \n",
      "2it [00:01,  1.98it/s]                                                                                                 \n",
      "1it [00:00,  7.01it/s]                                                                                                 \n",
      "1it [00:00,  1.16it/s]\n",
      "3it [00:03,  1.02s/it]                                                                                                 \n",
      "1it [00:01,  1.25s/it]\n",
      "1it [00:00,  4.42it/s]████████████████████████                                         | 21/42 [14:52<31:25, 89.80s/it]\n",
      "1it [00:00,  3.64it/s]\n",
      "4it [00:05,  1.33s/it]                                                                                                 \n",
      "2it [00:02,  1.11s/it]                                                                                                 \n",
      "2it [00:01,  1.07it/s]                                                                                                 \n",
      "1it [00:00, 17.59it/s]█████████████████████████████▊                                   | 24/42 [16:57<16:17, 54.33s/it]\n",
      "1it [00:00,  3.08it/s]\n",
      "2it [00:02,  1.24s/it]                                                                                                 \n",
      "1it [00:00,  2.66it/s]\n",
      "1it [00:00, 15.42it/s]███████████████████████████████▊                                 | 25/42 [17:30<13:32, 47.79s/it]\n",
      "1it [00:00, 20.05it/s]\n",
      "1it [00:00, 20.89it/s]\n",
      "1it [00:00,  2.51it/s]███████████████████████████████████▋                             | 27/42 [18:04<07:49, 31.32s/it]\n",
      "2it [00:01,  1.39it/s]                                                                                                 \n",
      "1it [00:00,  2.52it/s]█████████████████████████████████████▋                           | 28/42 [18:12<05:41, 24.38s/it]\n",
      "2it [00:02,  1.02s/it]                                                                                                 \n",
      "2it [00:01,  1.37it/s]                                                                                                 \n",
      "3it [00:01,  1.77it/s]                                                                                                 \n",
      "1it [00:00, 16.44it/s]\n",
      "1it [00:00,  2.55it/s]███████████████████████████████████████████▌                     | 31/42 [18:55<03:14, 17.67s/it]\n",
      "1it [00:00,  1.59it/s]\n",
      "1it [00:00,  2.08it/s]\n",
      "1it [00:00,  1.02it/s]█████████████████████████████████████████████▍                   | 32/42 [19:14<03:02, 18.26s/it]\n",
      "1it [00:00,  3.24it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.18it/s]\n",
      "1it [00:00,  2.34it/s]█████████████████████████████████████████████████▍               | 34/42 [19:33<01:51, 13.88s/it]\n",
      "1it [00:00,  6.66it/s]\n",
      "1it [00:00,  6.39it/s]\n",
      "1it [00:00,  2.60it/s]███████████████████████████████████████████████████▎             | 35/42 [19:44<01:30, 12.98s/it]\n",
      "1it [00:00,  3.66it/s]\n",
      "1it [00:00,  7.90it/s]\n",
      "1it [00:00, 15.42it/s]\n",
      "1it [00:00,  1.64it/s]█████████████████████████████████████████████████████▎           | 36/42 [20:04<01:29, 14.93s/it]\n",
      "1it [00:00,  7.72it/s]\n",
      "1it [00:00, 10.00it/s]\n",
      "1it [00:00, 20.89it/s]███████████████████████████████████████████████████████▏         | 37/42 [20:20<01:17, 15.45s/it]\n",
      "1it [00:00,  1.16it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "1it [00:00,  5.04it/s]\n",
      "2it [00:00,  3.13it/s]                                                                                                 \n",
      "1it [00:00, 20.06it/s]█████████████████████████████████████████████████████████████    | 40/42 [20:48<00:22, 11.49s/it]\n",
      "1it [00:00,  1.32it/s]███████████████████████████████████████████████████████████████  | 41/42 [20:50<00:08,  8.50s/it]\n",
      "1it [00:00,  3.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 42/42 [20:55<00:00, 29.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program done after 20.97 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "analyze('BABA',finviz,'finbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
